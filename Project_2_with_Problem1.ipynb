{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project_2 — Original Script (kept) + Problem 1 Add-on (sklearn, Gensim, PySpark)\n",
        "This notebook preserves the **same structure & variable names** as your original `Project_2` workflow for **Problem 2 (Clustering)**, and then **appends** the full **Problem 1 (Content-based Recommendation)** blocks at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# (Optional) Install packages if missing\n# !pip install gensim pyvi pyspark scikit-learn seaborn matplotlib pandas numpy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\n\ntry:\n    from pyvi.ViTokenizer import tokenize as vi_tokenize\nexcept Exception:\n    vi_tokenize = None\n\n# Load Vietnamese stopwords (same file name used in your project)\nSTOP_WORD_FILE = 'vietnamese-stopwords.txt'\ntry:\n    with open(STOP_WORD_FILE, 'r', encoding='utf-8') as f:\n        stop_words = [w.strip() for w in f.read().splitlines() if w.strip()]\nexcept FileNotFoundError:\n    stop_words = ['và', 'là', 'của', 'những', 'các', 'một', 'như', 'với', 'cho', 'đã', 'đang']\nprint(f\"Stopwords loaded: {len(stop_words)} terms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Load dataset (the original used 'data_motobikes.xlsx')\npath_candidates = ['data_motobikes.xlsx', 'data_motorbikes.xlsx']\npath = None\nfor p in path_candidates:\n    if os.path.exists(p):\n        path = p\n        break\nif path is None:\n    raise FileNotFoundError(\"Place data_motobikes.xlsx (or data_motorbikes.xlsx) in the working directory.\")\nprint(f\"Using: {path}\")\n\ndf = pd.read_excel(path, engine='openpyxl')\nprint(\"Shape trước xử lý:\", df.shape)\nprint(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Xử lý các cột liên quan giá và năm đăng ký\n\ndef clean_price(x):\n    if isinstance(x, str):\n        x = x.replace('.', '').replace(' đ', '').strip()\n        try:\n            return float(x)\n        except:\n            return np.nan\n    return x\n\ndef clean_range(x):\n    if isinstance(x, str):\n        x = x.replace(' tr', '').replace(',', '.').strip()\n        try:\n            return float(x) * 1_000_000\n        except:\n            return np.nan\n    return x\n\ndef clean_year(x):\n    if isinstance(x, str) and 'trước năm 1980' in x.lower():\n        return 1980\n    try:\n        return int(x)\n    except:\n        return np.nan\n\nif 'Giá' in df.columns:\n    df['Giá'] = df['Giá'].apply(clean_price)\nif 'Khoảng giá min' in df.columns:\n    df['Khoảng giá min'] = df['Khoảng giá min'].apply(clean_range)\nif 'Khoảng giá max' in df.columns:\n    df['Khoảng giá max'] = df['Khoảng giá max'].apply(clean_range)\nif 'Năm đăng ký' in df.columns:\n    df['Năm đăng ký'] = df['Năm đăng ký'].apply(clean_year)\n\n# Loại outlier theo IQR cho cột Giá\nQ1 = df['Giá'].quantile(0.25)\nQ3 = df['Giá'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound, upper_bound = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n\ndf_clean = df[(df['Giá'] >= lower_bound) & (df['Giá'] <= upper_bound)].copy()\n\n# Điền thiếu\ncategorical_cols = [c for c in df_clean.columns if df_clean[c].dtype == 'object']\nnumeric_cols = [c for c in df_clean.columns if pd.api.types.is_numeric_dtype(df_clean[c])]\nfor c in numeric_cols:\n    df_clean[c] = df_clean[c].fillna(df_clean[c].median())\nfor c in categorical_cols:\n    df_clean[c] = df_clean[c].fillna('Không rõ')\n\nprint(\"Sau làm sạch:\", df_clean.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nCURRENT_YEAR = 2025\nif 'Năm đăng ký' in df_clean.columns:\n    df_clean['Tuổi của xe'] = CURRENT_YEAR - df_clean['Năm đăng ký']\nelse:\n    df_clean['Tuổi của xe'] = np.nan\n\ndf_fe = df_clean.copy()\n\n# Chuẩn hoá text tương tự bản gốc\ntext_cols = ['Tiêu đề','Mô tả chi tiết','Thương hiệu','Địa chỉ','Dòng xe']\n\ndef normalize_text(text):\n    if not isinstance(text, str):\n        return ''\n    text = text.lower()\n    if vi_tokenize:\n        text = vi_tokenize(text)\n    text = re.sub(r'\\W+', ' ', text)\n    text = ' '.join([w for w in text.split() if w not in stop_words])\n    return text.strip()\n\nfor col in text_cols:\n    if col in df_fe.columns:\n        df_fe[f'{col}_normalized'] = df_fe[col].apply(normalize_text)\n\n# Kết hợp nội dung\nparts = []\nfor col in ['Tiêu đề_normalized','Mô tả chi tiết_normalized','Thương hiệu_normalized','Địa chỉ_normalized','Dòng xe_normalized']:\n    parts.append(df_fe[col] if col in df_fe.columns else '')\ndf_fe['Text_combined'] = (\n    (parts[0] if len(parts)>0 else '') + ' ' +\n    (parts[1] if len(parts)>1 else '') + ' ' +\n    (parts[2] if len(parts)>2 else '') + ' ' +\n    (parts[3] if len(parts)>3 else '') + ' ' +\n    (parts[4] if len(parts)>4 else '')\n)\n\n# Numeric engineering giống bản gốc\nif all(c in df_fe.columns for c in ['Khoảng giá min','Khoảng giá max']):\n    df_fe['Giá trung bình'] = (df_fe['Khoảng giá min'] + df_fe['Khoảng giá max'])/2\nelse:\n    df_fe['Giá trung bình'] = np.nan\n\nif 'Giá' in df_fe.columns:\n    df_fe['Tỷ lệ giá'] = df_fe['Giá'] / df_fe['Giá trung bình'].replace(0, np.nan)\nelse:\n    df_fe['Tỷ lệ giá'] = np.nan\n\nif 'Số Km đã đi' in df_fe.columns:\n    df_fe['Km per year'] = df_fe['Số Km đã đi'] / (df_fe['Tuổi của xe'] + 1e-5)\n    df_fe['Số Km đã đi_log'] = np.log1p(df_fe['Số Km đã đi'])\nelse:\n    df_fe['Km per year'] = np.nan\n    df_fe['Số Km đã đi_log'] = np.nan\n\nif 'Giá' in df_fe.columns:\n    df_fe['Giá_log'] = np.log1p(df_fe['Giá'])\nelse:\n    df_fe['Giá_log'] = np.nan\n\n# LabelEncoder cho một số cột phân loại\nfor col in ['Thương hiệu','Dòng xe','Loại xe','Dung tích xe','Xuất xứ']:\n    if col in df_fe.columns:\n        le = LabelEncoder()\n        df_fe[f'{col}_label'] = le.fit_transform(df_fe[col].astype(str))\n\n# Chuẩn hoá cột số\nnum_cols = [c for c in ['Giá','Khoảng giá min','Khoảng giá max','Năm đăng ký','Số Km đã đi','Tuổi của xe','Tỷ lệ giá','Km per year','Giá_log','Số Km đã đi_log'] if c in df_fe.columns]\nscaler = StandardScaler()\ndf_fe[num_cols] = scaler.fit_transform(df_fe[num_cols])\n\nprint('Feature set ready:', df_fe.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Text vector hoá cho clustering (SVD 100 chiều như trong bản gốc)\ntext_vect = TfidfVectorizer(max_features=1000, ngram_range=(1,2), analyzer='word', stop_words=stop_words, min_df=2)\nX_text = text_vect.fit_transform(df_fe['Text_combined'].fillna(''))\nsvd = TruncatedSVD(n_components=100, random_state=42)\nX_text_r = svd.fit_transform(X_text)\n\n# Kết hợp với nhãn + số\nlabel_cols = [c for c in df_fe.columns if c.endswith('_label')]\nnum_cols = [c for c in ['Giá','Khoảng giá min','Khoảng giá max','Năm đăng ký','Số Km đã đi','Tuổi của xe','Tỷ lệ giá','Km per year','Giá_log','Số Km đã đi_log'] if c in df_fe.columns]\nX_others = df_fe[label_cols + num_cols].values\n\nX = np.hstack([X_text_r, X_others])\nX = np.nan_to_num(X)\n\n# Tìm K tối ưu theo silhouette cho KMeans\nk_range = range(2, 11)\nsil_scores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X)\n    sil_scores.append(silhouette_score(X, km.labels_))\nopt_k = k_range[int(np.argmax(sil_scores))]\nprint('Optimal K (KMeans):', opt_k)\n\nkmeans = KMeans(n_clusters=opt_k, random_state=42, n_init=10)\nkmeans_labels = kmeans.fit_predict(X)\nkm_sil = silhouette_score(X, kmeans_labels)\nkm_db  = davies_bouldin_score(X, kmeans_labels)\n\n# GMM\nbest_g, best_g_sil = None, -1\nfor g in k_range:\n    gmm = GaussianMixture(n_components=g, random_state=42)\n    gmm.fit(X)\n    s = silhouette_score(X, gmm.predict(X))\n    if s > best_g_sil:\n        best_g_sil, best_g = s, g\n\ngmm = GaussianMixture(n_components=best_g, random_state=42)\ngmm_labels = gmm.fit_predict(X)\ngmm_sil = silhouette_score(X, gmm_labels)\ngmm_db  = davies_bouldin_score(X, gmm_labels)\n\n# Agglomerative dùng cùng opt_k\nagg = AgglomerativeClustering(n_clusters=opt_k, linkage='ward')\nagg_labels = agg.fit_predict(X)\nagg_sil = silhouette_score(X, agg_labels)\nagg_db  = davies_bouldin_score(X, agg_labels)\n\nprint(pd.DataFrame({'Model':['KMeans','GMM','Agglomerative'],\n                    'Silhouette':[km_sil, gmm_sil, agg_sil],\n                    'Davies-Bouldin':[km_db, gmm_db, agg_db]})\n      .sort_values('Silhouette', ascending=False))\n\n# Gắn nhãn tốt nhất để profiling (KMeans)\ndf_fe['cluster_kmeans'] = kmeans_labels\nprofile = (df_fe.groupby('cluster_kmeans')\n           .agg({'Giá':'mean', 'Số Km đã đi':'mean', 'Tuổi của xe':'mean', 'Thương hiệu':lambda x: x.mode()[0] if len(x)>0 else 'NA', 'Loại xe':lambda x: x.mode()[0] if len(x)>0 else 'NA'})\n           .rename(columns={'Giá':'Giá TB','Số Km đã đi':'Km TB','Tuổi của xe':'Tuổi TB'}))\nprint(profile)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# PySpark clustering theo bản gốc\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer, VectorAssembler\nfrom pyspark.ml.clustering import KMeans as SKMeans, GaussianMixture as SGMM, BisectingKMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.sql.functions import col, monotonically_increasing_id\n\nspark = SparkSession.builder.appName(\"MarketSegmentation\").getOrCreate()\n\n# Build Spark text TF-IDF\nsdf = spark.createDataFrame(df_fe[['id','Text_combined']].copy())\n_tok = Tokenizer(inputCol='Text_combined', outputCol='tokens')\nsdf = _tok.transform(sdf)\n_rem = StopWordsRemover(inputCol='tokens', outputCol='filtered', stopWords=stop_words)\nsdf = _rem.transform(sdf)\n_htf = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=1<<14)\nsdf = _htf.transform(sdf)\n_idf = IDF(inputCol='rawFeatures', outputCol='features')\nidf_model = _idf.fit(sdf)\nsdf = idf_model.transform(sdf)\n_norm = Normalizer(inputCol='features', outputCol='normFeatures')\nsdf = _norm.transform(sdf)\n\n# Assemble with numeric/label columns\nextra_cols = []\nfor c in ['Giá','Khoảng giá min','Khoảng giá max','Năm đăng ký','Số Km đã đi','Tuổi của xe','Tỷ lệ giá','Km per year','Giá_log','Số Km đã đi_log',\n          'Thương hiệu_label','Dòng xe_label','Loại xe_label','Dung tích xe_label','Xuất xứ_label']:\n    if c in df_fe.columns:\n        extra_cols.append(c)\n\nsdf_extra = spark.createDataFrame(df_fe[extra_cols])\nsdf_feat = sdf.select('normFeatures').withColumnRenamed('normFeatures','textFeat')\n\nsdf_feat = sdf_feat.withColumn('row_id', monotonically_increasing_id())\nsdf_extra = sdf_extra.withColumn('row_id', monotonically_increasing_id())\nsdf_join = sdf_feat.join(sdf_extra, on='row_id').drop('row_id')\n\nassembler = VectorAssembler(inputCols=['textFeat'] + [c for c in sdf_extra.columns if c!='row_id'], outputCol='features')\nsdf_vec = assembler.transform(sdf_join)\n\nevaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features')\n\n# KMeans\nscores = []\nfor k in range(2,11):\n    m = SKMeans(k=k, seed=42).fit(sdf_vec)\n    p = m.transform(sdf_vec)\n    scores.append((k, evaluator.evaluate(p)))\nopt_k = max(scores, key=lambda x: x[1])[0]\nkm_model = SKMeans(k=opt_k, seed=42).fit(sdf_vec)\npreds_km = km_model.transform(sdf_vec)\nkm_sil = evaluator.evaluate(preds_km)\n\n# BisectingKMeans\nscores_b = []\nfor k in range(2,11):\n    m = BisectingKMeans(k=k, seed=42).fit(sdf_vec)\n    p = m.transform(sdf_vec)\n    scores_b.append((k, evaluator.evaluate(p)))\nopt_b = max(scores_b, key=lambda x: x[1])[0]\nbkm_model = BisectingKMeans(k=opt_b, seed=42).fit(sdf_vec)\npreds_b = bkm_model.transform(sdf_vec)\nbkm_sil = evaluator.evaluate(preds_b)\n\n# GMM (có thể silhouette âm với dữ liệu sparse)\ntry:\n    gmm_model = SGMM(k=2, seed=42).fit(sdf_vec)\n    preds_g = gmm_model.transform(sdf_vec)\n    gmm_sil = evaluator.evaluate(preds_g)\nexcept Exception:\n    gmm_sil = float('nan')\n\nprint(pd.DataFrame({'Model':['KMeans','BisectingKMeans','GMM'], 'Silhouette':[km_sil, bkm_sil, gmm_sil]}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n---\n\n# Problem 1 — Content-Based Recommendation (Sklearn + Gensim + PySpark)\n**Lưu ý:** Các cell dưới đây **không thay đổi** code gốc, chỉ **bổ sung** hàm gợi ý theo yêu cầu Topic (cosine_similarity & gensim) và phiên bản PySpark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nassert 'df_fe' in globals(), \"df_fe chưa sẵn sàng. Hãy chạy phần gốc trước.\"\nassert 'Text_combined' in df_fe.columns, \"Thiếu cột Text_combined từ phần Feature Engineering.\"\n\n# Heuristic chọn cấu hình tốt nhất: brand-consistency\ndef brand_consistency(sim_matrix, brands, topk=10):\n    n = sim_matrix.shape[0]\n    if brands is None:\n        return 0.0\n    same, total = 0, 0\n    for i in range(n):\n        sims = sim_matrix[i]\n        idx = np.argpartition(-sims, range(1, topk+1))[1:topk+1]\n        base = brands[i]\n        total += len(idx)\n        same += sum(1 for j in idx if brands[j] == base)\n    return same / max(total, 1)\n\ntexts = df_fe['Text_combined'].fillna('')\nbrands = df_fe['Thương hiệu'] if 'Thương hiệu' in df_fe.columns else None\n\nmax_features_grid = [3000, 5000, 8000]\nngram_grid = [(1,1), (1,2)]\nsvd_components = 200\n\nbest_score = -1\nbest_cfg = None\nbest_sim = None\nbest_vectorizer = None\nbest_svd = None\n\nfor mf in max_features_grid:\n    for ng in ngram_grid:\n        vect = TfidfVectorizer(max_features=mf, ngram_range=ng, analyzer='word', stop_words=stop_words, min_df=2)\n        X = vect.fit_transform(texts)\n        if X.shape[1] > svd_components:\n            svd = TruncatedSVD(n_components=svd_components, random_state=42)\n            Xr = svd.fit_transform(X)\n            sim = cosine_similarity(Xr)\n        else:\n            svd = None\n            sim = cosine_similarity(X)\n        score = brand_consistency(sim, brands)\n        if score > best_score:\n            best_score = score\n            best_cfg = (mf, ng)\n            best_sim = sim\n            best_vectorizer = vect\n            best_svd = svd\n\nprint(f\"[Problem1][sklearn] Best TF-IDF: max_features={best_cfg[0]}, ngram={best_cfg[1]}, brand-consistency={best_score:.3f}\")\n\n# Helper để recommend theo id\n\ndef _get_index_by_id(df, item_id, id_col='id'):\n    idx = df.index[df[id_col] == item_id].tolist()\n    if not idx:\n        raise ValueError(f\"Item id {item_id} không tồn tại ở cột '{id_col}'\")\n    return idx[0]\n\ndef recommend_by_id_sklearn(df_source, sim_matrix, item_id, topn=5, id_col='id', cols=('id','Tiêu đề','Thương hiệu','Giá')):\n    i = _get_index_by_id(df_source, item_id, id_col=id_col)\n    sims = sim_matrix[i].copy()\n    order = np.argsort(-sims)\n    order = [j for j in order if j != i][:topn]\n    out = df_source.iloc[order].copy()\n    out['similarity'] = sims[order]\n    keep_cols = [c for c in cols if c in out.columns] + ['similarity']\n    return out[keep_cols]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nfrom gensim import corpora, models, similarities\n\ntexts = [str(x) for x in df_fe['Text_combined'].fillna('')]\ncontent_tokens = [t.split() for t in texts]\ncontent_tokens = [[w for w in doc if w not in stop_words] for doc in content_tokens]\n\ndictionary = corpora.Dictionary(content_tokens)\ncorpus = [dictionary.doc2bow(doc) for doc in content_tokens]\nmodel_tfidf = models.TfidfModel(corpus)\nindex = similarities.SparseMatrixSimilarity(model_tfidf[corpus], num_features=len(dictionary))\n\n\ndef recommend_by_id_gensim(df_source, item_id, topn=5, id_col='id', cols=('id','Tiêu đề','Thương hiệu','Giá')):\n    i = df_source.index[df_source[id_col] == item_id].tolist()\n    if not i:\n        raise ValueError(f\"Item id {item_id} không tồn tại\")\n    i = i[0]\n    vec = dictionary.doc2bow(content_tokens[i])\n    sims = index[model_tfidf[vec]]\n    order = np.argsort(-sims)\n    order = [j for j in order if j != i][:topn]\n    out = df_source.iloc[order].copy()\n    out['similarity'] = sims[order]\n    keep_cols = [c for c in cols if c in out.columns] + ['similarity']\n    return out[keep_cols]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer\nfrom pyspark.ml.feature import BucketedRandomProjectionLSH\n\nspark = SparkSession.builder.appName(\"Problem1_ContentBased\").getOrCreate()\n\nuse_cols = ['id','Tiêu đề','Thương hiệu','Giá','Text_combined']\nuse_cols = [c for c in use_cols if c in df_fe.columns]\nsdf_cb = spark.createDataFrame(df_fe[use_cols])\n\n_tok2 = Tokenizer(inputCol='Text_combined', outputCol='tokens')\nsdf_cb = _tok2.transform(sdf_cb)\n_rem2 = StopWordsRemover(inputCol='tokens', outputCol='filtered', stopWords=stop_words)\nsdf_cb = _rem2.transform(sdf_cb)\n_htf2 = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=1<<14)\nsdf_cb = _htf2.transform(sdf_cb)\n_idf2 = IDF(inputCol='rawFeatures', outputCol='features')\nidf_model2 = _idf2.fit(sdf_cb)\nsdf_cb = idf_model2.transform(sdf_cb)\n_norm2 = Normalizer(inputCol='features', outputCol='normFeatures')\nsdf_cb = _norm2.transform(sdf_cb)\n\n_lsh2 = BucketedRandomProjectionLSH(inputCol='normFeatures', outputCol='hashes', bucketLength=2.0, numHashTables=5)\nlsh_model2 = _lsh2.fit(sdf_cb)\n\n\ndef recommend_by_id_spark(item_id, topn=5):\n    item = sdf_cb.filter(col('id') == item_id).limit(1)\n    if item.count() == 0:\n        raise ValueError(f\"Item id {item_id} không tồn tại\")\n    res = lsh_model2.approxNearestNeighbors(sdf_cb, item.collect()[0]['normFeatures'], topn+1)\n    res = res.filter(col('id') != item_id).orderBy(col('distCol').asc())\n    return res.select('id','Tiêu đề','Thương hiệu','Giá','distCol').toPandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Usage\n",
        "```python\n",
        "# Ví dụ: lấy id đầu tiên\n",
        "item_id = int(df_fe['id'].iloc[0])\n",
        "\n",
        "# Sklearn (cosine)\n",
        "recommend_by_id_sklearn(df_fe, best_sim, item_id, topn=5)\n",
        "\n",
        "# Gensim\n",
        "recommend_by_id_gensim(df_fe, item_id, topn=5)\n",
        "\n",
        "# PySpark\n",
        "recommend_by_id_spark(item_id, topn=5)\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}