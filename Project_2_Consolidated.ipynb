{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project 2 — Content-Based Recommendation & Market Segmentation (Consolidated)\n",
        "This notebook consolidates **Problem 1** (Content-based Recommendation using **Cosine similarity** and **Gensim**) and **Problem 2** (Market Segmentation / **Clustering**) in **two environments**:\n",
        "- **Scikit-learn / Python (local ML)**\n",
        "- **PySpark (distributed ML)**\n",
        "\n",
        "Data source: `data_motobikes.xlsx` (or `data_motorbikes.xlsx` fallback).\n",
        "\n",
        "> Notes: Run the install cell if your environment lacks the required libraries (e.g. `gensim`, `pyvi`, `pyspark`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# If running on a fresh environment, uncomment to install packages\n# !pip install gensim pyvi pyspark scikit-learn seaborn matplotlib pandas numpy\n\nimport os, warnings, re, json\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nfrom sklearn.decomposition import PCA\n\n# Gensim (content-based alt)\nfrom gensim import corpora, models, similarities\n\n# Vietnamese tokenization (optional)\ntry:\n    from pyvi.ViTokenizer import tokenize as vi_tokenize\nexcept Exception:\n    vi_tokenize = None\n\n# Try to load stop words if present\nSTOP_WORD_FILE = \"vietnamese-stopwords.txt\"\nstop_words = []\nif os.path.exists(STOP_WORD_FILE):\n    with open(STOP_WORD_FILE, 'r', encoding='utf-8') as f:\n        stop_words = [w.strip() for w in f.read().splitlines() if w.strip()]\nelse:\n    # Minimal fallback (extend as needed)\n    stop_words = ['và', 'là', 'của', 'những', 'các', 'một', 'như', 'với', 'cho', 'đã', 'đang']\nprint(f\"Loaded stop words: {len(stop_words)} terms from {STOP_WORD_FILE} (exists={os.path.exists(STOP_WORD_FILE)})\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Load dataset from Excel (Chợ Tốt motorbikes ~7000+ rows)\nexcel_paths = [\n    'data_motobikes.xlsx',  # common name from prior work\n    'data_motorbikes.xlsx'  # name in topic PDF\n]\npath = None\nfor p in excel_paths:\n    if os.path.exists(p):\n        path = p\n        break\nif path is None:\n    raise FileNotFoundError(\"Neither data_motobikes.xlsx nor data_motorbikes.xlsx found in working directory.\")\n\nprint(f\"Using dataset: {path}\")\ndf = pd.read_excel(path, engine='openpyxl')\nprint(df.shape)\ndf.head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# --- Basic cleaning/typing ---\nimport numpy as np\n\ndef clean_price(x):\n    if isinstance(x, str):\n        x = x.replace('.', '').replace(' đ', '').strip()\n        try:\n            return float(x)\n        except:\n            return np.nan\n    return x\n\ndef clean_range(x):\n    # \"72.53 tr\" -> 72.53 * 1e6\n    if isinstance(x, str):\n        x = x.replace(' tr', '').replace(',', '.').strip()\n        try:\n            return float(x) * 1_000_000\n        except:\n            return np.nan\n    return x\n\ndef clean_year(x):\n    if isinstance(x, str) and 'trước năm 1980' in x.lower():\n        return 1980\n    try:\n        return int(x)\n    except:\n        return np.nan\n\nfor col in df.columns:\n    if col.lower() == 'giá':\n        df[col] = df[col].apply(clean_price)\n    elif col.lower() == 'khoảng giá min':\n        df[col] = df[col].apply(clean_range)\n    elif col.lower() == 'khoảng giá max':\n        df[col] = df[col].apply(clean_range)\n    elif col.lower() == 'năm đăng ký':\n        df[col] = df[col].apply(clean_year)\n\n# Remove price outliers by IQR rule\nQ1 = df['Giá'].quantile(0.25)\nQ3 = df['Giá'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n\ndf_clean = df[(df['Giá'] >= lower_bound) & (df['Giá'] <= upper_bound)].copy()\n\n# Impute\ncategorical_cols = [c for c in df_clean.columns if df_clean[c].dtype == 'object']\nnumeric_cols = [c for c in df_clean.columns if pd.api.types.is_numeric_dtype(df_clean[c])]\nfor col in numeric_cols:\n    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\nfor col in categorical_cols:\n    df_clean[col] = df_clean[col].fillna('Không rõ')\n\nCURRENT_YEAR = datetime.now().year\nif 'Năm đăng ký' in df_clean.columns:\n    df_clean['Tuổi của xe'] = CURRENT_YEAR - df_clean['Năm đăng ký']\nelse:\n    df_clean['Tuổi của xe'] = np.nan\n\n# --- Text normalization ---\ntext_cols_base = ['Tiêu đề', 'Mô tả chi tiết', 'Thương hiệu', 'Địa chỉ', 'Dòng xe']\n\ndef normalize_text(text):\n    if not isinstance(text, str):\n        return ''\n    text = text.lower()\n    # tokenize with pyvi if available\n    if vi_tokenize:\n        text = vi_tokenize(text)\n    # remove non-word chars\n    text = re.sub(r'\\W+', ' ', text)\n    # remove stop words\n    text = ' '.join([w for w in text.split() if w not in stop_words])\n    return text.strip()\n\nfor col in text_cols_base:\n    if col in df_clean.columns:\n        df_clean[f'{col}_normalized'] = df_clean[col].apply(normalize_text)\n\n# Combined text feature\ndf_clean['Text_combined'] = (\n    df_clean.get('Tiêu đề_normalized', '') + ' ' +\n    df_clean.get('Mô tả chi tiết_normalized', '') + ' ' +\n    df_clean.get('Thương hiệu_normalized', '') + ' ' +\n    df_clean.get('Địa chỉ_normalized', '') + ' ' +\n    df_clean.get('Dòng xe_normalized', '')\n)\n\n# Feature engineering (numerical)\ndf_fe = df_clean.copy()\ndf_fe['Giá trung bình'] = (df_fe['Khoảng giá min'] + df_fe['Khoảng giá max']) / 2\n# Avoid zero div\nden = df_fe['Giá trung bình'].replace(0, np.nan)\ndf_fe['Tỷ lệ giá'] = df_fe['Giá'] / den\n# km/year\nif 'Số Km đã đi' in df_fe.columns:\n    df_fe['Km per year'] = df_fe['Số Km đã đi'] / (df_fe['Tuổi của xe'] + 1e-5)\nelse:\n    df_fe['Km per year'] = np.nan\n\n# log transforms\nfor c in ['Giá', 'Số Km đã đi']:\n    if c in df_fe.columns:\n        df_fe[f'{c}_log'] = np.log1p(df_fe[c])\n\n# Scale numeric features\nnum_cols = [c for c in ['Giá','Khoảng giá min','Khoảng giá max','Năm đăng ký','Số Km đã đi','Tuổi của xe','Tỷ lệ giá','Km per year','Giá_log','Số Km đã đi_log'] if c in df_fe.columns]\nscaler = StandardScaler()\ndf_fe[num_cols] = scaler.fit_transform(df_fe[num_cols])\n\nprint(\"Cleaned shape:\", df_fe.shape)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Problem 1 — Content-Based Recommendation (Scikit-learn & Gensim)\nWe build item embeddings from textual attributes using **TF–IDF** and compute **cosine similarity** for top-N similar bikes. In parallel, we also build a **Gensim** TF–IDF + **SparseMatrixSimilarity** index as an alternative.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# --- TF–IDF with scikit-learn ---\nmax_features_grid = [3000, 5000]\nngram_grid = [(1,1), (1,2)]\nsvd_components = 200  # reduce dim for efficiency\n\n# Heuristic metric to tune text params (no interaction data):\n# For each item, compute top-10 similar items and measure brand consistency (% same brand)\ndef brand_consistency(sim_matrix, brands, topk=10):\n    n = sim_matrix.shape[0]\n    if brands is None:\n        return 0.0\n    same = 0\n    total = 0\n    for i in range(n):\n        sims = sim_matrix[i]\n        idx = np.argpartition(-sims, range(1, topk+1))[1:topk+1]\n        base = brands[i]\n        total += len(idx)\n        same += sum(1 for j in idx if brands[j] == base)\n    return same / max(total, 1)\n\nbest_score = -1\nbest_cfg = None\nbest_sim = None\nbest_vectorizer = None\n\nbrands = df_fe['Thương hiệu'] if 'Thương hiệu' in df_fe.columns else None\n\nfor mf in max_features_grid:\n    for ng in ngram_grid:\n        vect = TfidfVectorizer(max_features=mf, ngram_range=ng, analyzer='word', stop_words=stop_words, min_df=2)\n        X = vect.fit_transform(df_fe['Text_combined'].fillna(''))\n        # Optional dimensionality reduction\n        if svd_components and X.shape[1] > svd_components:\n            svd = TruncatedSVD(n_components=svd_components, random_state=42)\n            Xr = svd.fit_transform(X)\n            # cosine on reduced\n            sim = cosine_similarity(Xr)\n        else:\n            sim = cosine_similarity(X)\n        score = brand_consistency(sim, brands)\n        if score > best_score:\n            best_score = score\n            best_cfg = (mf, ng)\n            best_sim = sim\n            best_vectorizer = vect\n\nprint(f\"Best TF-IDF cfg: max_features={best_cfg[0]}, ngram={best_cfg[1]} | brand_consistency={best_score:.3f}\")\n\n# Recommendation function using best_sim\ndef recommend_by_id(df_source, sim_matrix, id_col='id', item_id=None, topn=5):\n    if item_id is None:\n        raise ValueError('Provide item_id')\n    if id_col not in df_source.columns:\n        raise ValueError(f'{id_col} not in DataFrame')\n    indices = df_source.index[df_source[id_col] == item_id].tolist()\n    if not indices:\n        raise ValueError(f'Item id {item_id} not found')\n    i = indices[0]\n    sims = sim_matrix[i]\n    # Top-N excluding itself\n    idx = np.argsort(-sims)\n    idx = [j for j in idx if j != i][:topn]\n    return df_source.iloc[idx]\n\n# Example\nif 'id' in df_fe.columns:\n    try:\n        example_id = int(df_fe['id'].iloc[0])\n    except:\n        example_id = df_fe.index[0]\n    recs = recommend_by_id(df_fe, best_sim, item_id=example_id, topn=5)\n    recs[['id','Tiêu đề','Thương hiệu','Giá']].head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# --- Gensim TF–IDF + SparseMatrixSimilarity ---\n# Build tokenized content\ntexts = [str(x) for x in df_fe['Text_combined'].fillna('')]\n# Simple tokenization (pyvi tokenization already applied in normalize_text)\n# Convert to lists for gensim\ncontent_tokens = [t.split() for t in texts]\n# Remove stopwords\ncontent_tokens = [[w for w in doc if w not in stop_words] for doc in content_tokens]\n\n# Dictionary & corpus\ndictionary = corpora.Dictionary(content_tokens)\ncorpus = [dictionary.doc2bow(doc) for doc in content_tokens]\n\n# TF-IDF model & similarity index\nmodel_tfidf = models.TfidfModel(corpus)\nindex = similarities.SparseMatrixSimilarity(model_tfidf[corpus], num_features=len(dictionary))\n\n# Convert index to dense similarity (WARNING: may be large for big datasets). Here we only compute top-N for a single item.\n\ndef gensim_recommend(df_source, item_id, topn=5):\n    indices = df_source.index[df_source['id'] == item_id].tolist()\n    if not indices:\n        raise ValueError(f'Item id {item_id} not found')\n    i = indices[0]\n    vec = dictionary.doc2bow(content_tokens[i])\n    sims = index[model_tfidf[vec]]  # similarities against all docs\n    # Top-N excluding itself\n    order = np.argsort(-sims)\n    order = [j for j in order if j != i][:topn]\n    return df_source.iloc[order]\n\n# Example usage\nif 'id' in df_fe.columns:\n    try:\n        example_id = int(df_fe['id'].iloc[0])\n    except:\n        example_id = df_fe.index[0]\n    recs_gem = gensim_recommend(df_fe, item_id=example_id, topn=5)\n    recs_gem[['id','Tiêu đề','Thương hiệu','Giá']].head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n### Problem 1 — Content-Based Recommendation (PySpark)\nVectorize content with Spark ML (Tokenizer → StopWordsRemover → HashingTF → IDF → Normalizer), then use **BucketedRandomProjectionLSH** (approximate NN on normalized vectors) to retrieve top-N similar items. On normalized vectors, Euclidean distance correlates with cosine similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import ArrayType, StringType, DoubleType\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import BucketedRandomProjectionLSH\n\nspark = SparkSession.builder.appName(\"ContentBasedRecommendation\").getOrCreate()\n\n# Create Spark DataFrame with necessary columns\nuse_cols = ['id','Tiêu đề','Thương hiệu','Giá','Text_combined']\nuse_cols = [c for c in use_cols if c in df_fe.columns]\nsdf = spark.createDataFrame(df_fe[use_cols])\n\n# Tokenize (content is already normalized; still break into tokens)\ntok = Tokenizer(inputCol='Text_combined', outputCol='tokens')\nsdf_tok = tok.transform(sdf)\n\n# Stop words (Vietnamese)\nremover = StopWordsRemover(inputCol='tokens', outputCol='filtered', stopWords=stop_words)\nsdf_sw = remover.transform(sdf_tok)\n\n# HashingTF → IDF\nhtf = HashingTF(inputCol='filtered', outputCol='rawFeatures', numFeatures=1<<14)\nsdf_htf = htf.transform(sdf_sw)\nidf = IDF(inputCol='rawFeatures', outputCol='features')\nidf_model = idf.fit(sdf_htf)\nsdf_tfidf = idf_model.transform(sdf_htf)\n\n# Normalize to unit length (approximate cosine via euclidean)\nnorm = Normalizer(inputCol='features', outputCol='normFeatures')\nsdf_norm = norm.transform(sdf_tfidf)\n\n# LSH for approximate nearest neighbors\nlsh = BucketedRandomProjectionLSH(inputCol='normFeatures', outputCol='hashes', bucketLength=2.0, numHashTables=5)\nlsh_model = lsh.fit(sdf_norm)\n\n# Function to recommend top-N similar for a given id\ndef spark_recommend(item_id, topn=5):\n    item = sdf_norm.filter(col('id') == item_id).limit(1)\n    if item.count() == 0:\n        raise ValueError(f'Item id {item_id} not found')\n    # approxNearestNeighbors uses Euclidean distance; on normalized vectors it correlates with cosine similarity\n    result = lsh_model.approxNearestNeighbors(sdf_norm, item.collect()[0]['normFeatures'], topn+1)\n    # Exclude itself\n    result = result.filter(col('id') != item_id).orderBy(col('distCol').asc())\n    return result.select('id','Tiêu đề','Thương hiệu','Giá','distCol').toPandas()\n\n# Example\nexample_id = int(df_fe['id'].iloc[0]) if 'id' in df_fe.columns else int(df_fe.index[0])\nspark_recs = spark_recommend(example_id, topn=5)\nspark_recs.head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Problem 2 — Market Segmentation (Clustering)\nWe cluster items in both environments (**Scikit-learn** and **PySpark**) and compare using **Silhouette** and **Davies–Bouldin**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Build text embeddings from best TF-IDF (re-use best_vectorizer)\nX_text = best_vectorizer.transform(df_fe['Text_combined'].fillna(''))\nsvd_components = 100\nif X_text.shape[1] > svd_components:\n    svd2 = TruncatedSVD(n_components=svd_components, random_state=42)\n    X_text_r = svd2.fit_transform(X_text)\nelse:\n    X_text_r = X_text.toarray()\n\n# Gather categorical encodings\nlabel_cols = []\nfor col in ['Thương hiệu','Dòng xe','Loại xe','Dung tích xe','Xuất xứ']:\n    if col in df_fe.columns:\n        le = LabelEncoder()\n        df_fe[f'{col}_label'] = le.fit_transform(df_fe[col].astype(str))\n        label_cols.append(f'{col}_label')\n\nnum_cols = [c for c in ['Giá','Khoảng giá min','Khoảng giá max','Năm đăng ký','Số Km đã đi','Tuổi của xe','Tỷ lệ giá','Km per year','Giá_log','Số Km đã đi_log'] if c in df_fe.columns]\nX_others = df_fe[label_cols + num_cols].values\nX = np.hstack([X_text_r, X_others])\nX = np.nan_to_num(X)\n\n# Evaluate KMeans/GMM/Agglomerative\nk_range = range(2, 11)\n\nsil_scores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X)\n    sil_scores.append(silhouette_score(X, km.labels_))\nopt_k = k_range[np.argmax(sil_scores)]\nprint(\"Optimal K (KMeans):\", opt_k)\n\nkmeans = KMeans(n_clusters=opt_k, random_state=42, n_init=10)\nkmeans_labels = kmeans.fit_predict(X)\nkm_sil = silhouette_score(X, kmeans_labels)\nkm_db  = davies_bouldin_score(X, kmeans_labels)\n\n# GMM\ngmm_sils = []\nfor n in k_range:\n    gmm = GaussianMixture(n_components=n, random_state=42)\n    gmm.fit(X)\n    gmm_sils.append(silhouette_score(X, gmm.predict(X)))\nopt_g = k_range[np.argmax(gmm_sils)]\ngmm = GaussianMixture(n_components=opt_g, random_state=42)\ngmm_labels = gmm.fit_predict(X)\ngmm_sil = silhouette_score(X, gmm_labels)\ngmm_db  = davies_bouldin_score(X, gmm_labels)\n\n# Agglomerative\nagg = AgglomerativeClustering(n_clusters=opt_k, linkage='ward')\nagg_labels = agg.fit_predict(X)\nagg_sil = silhouette_score(X, agg_labels)\nagg_db  = davies_bouldin_score(X, agg_labels)\n\npd.DataFrame({\n    'Model': ['KMeans','GMM','Agglomerative'],\n    'Silhouette': [km_sil, gmm_sil, agg_sil],\n    'Davies-Bouldin': [km_db, gmm_db, agg_db]\n}).sort_values('Silhouette', ascending=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans as SKMeans, GaussianMixture as SGMM, BisectingKMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\n# Reuse sdf_norm from recommendation section for text features (normFeatures)\n# Assemble numeric and label features as well\nextra_cols = []\nfor c in ['Giá','Khoảng giá min','Khoảng giá max','Năm đăng ký','Số Km đã đi','Tuổi của xe','Tỷ lệ giá','Km per year','Giá_log','Số Km đã đi_log']:\n    if c in df_fe.columns:\n        extra_cols.append(c)\nfor c in ['Thương hiệu_label','Dòng xe_label','Loại xe_label','Dung tích xe_label','Xuất xứ_label']:\n    if c in df_fe.columns:\n        extra_cols.append(c)\n\nsdf_extra = spark.createDataFrame(df_fe[extra_cols])\nsdf_feat = sdf_norm.select('normFeatures').withColumnRenamed('normFeatures','textFeat')\n# Combine columns back by row index\nfrom pyspark.sql.functions import monotonically_increasing_id\nsdf_feat = sdf_feat.withColumn('row_id', monotonically_increasing_id())\nsdf_extra = sdf_extra.withColumn('row_id', monotonically_increasing_id())\nsdf_join = sdf_feat.join(sdf_extra, on='row_id').drop('row_id')\n\n# VectorAssembler (requires numeric vector cols)\n# textFeat already vector; assembler can combine via inputCols with textFeat and numeric columns\nassembler = VectorAssembler(inputCols=['textFeat'] + [c for c in sdf_extra.columns if c != 'row_id'], outputCol='features')\nsdf_vec = assembler.transform(sdf_join)\n\n# Evaluate K range\nevaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features')\nsil_scores = []\nfor k in range(2, 11):\n    km = SKMeans(k=k, seed=42)\n    model = km.fit(sdf_vec)\n    preds = model.transform(sdf_vec)\n    sil = evaluator.evaluate(preds)\n    sil_scores.append((k, sil))\nopt_k = max(sil_scores, key=lambda x: x[1])[0]\nprint('Optimal K (Spark KMeans):', opt_k)\n\nkm_model = SKMeans(k=opt_k, seed=42).fit(sdf_vec)\npreds_km = km_model.transform(sdf_vec)\nkm_sil = evaluator.evaluate(preds_km)\n\n# Bisecting KMeans\nbkm_scores = []\nfor k in range(2,11):\n    bkm = BisectingKMeans(k=k, seed=42)\n    m = bkm.fit(sdf_vec)\n    p = m.transform(sdf_vec)\n    bkm_scores.append((k, evaluator.evaluate(p)))\nopt_b = max(bkm_scores, key=lambda x: x[1])[0]\nbkm_model = BisectingKMeans(k=opt_b, seed=42).fit(sdf_vec)\npreds_b = bkm_model.transform(sdf_vec)\nbkm_sil = evaluator.evaluate(preds_b)\n\n# GMM often yields negative silhouette on sparse text; still include for completeness\nopt_g = 2\ntry:\n    gmm_model = SGMM(k=opt_g, seed=42).fit(sdf_vec)\n    preds_g = gmm_model.transform(sdf_vec)\n    gmm_sil = evaluator.evaluate(preds_g)\nexcept Exception as e:\n    gmm_sil = float('nan')\n\npd.DataFrame({'Model':['KMeans','BisectingKMeans','GMM'], 'Silhouette':[km_sil, bkm_sil, gmm_sil]})\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n### Summary & Next Steps\n- **Problem 1 (Content-based)**: Implemented **scikit-learn cosine** with parameter tuning (brand-consistency heuristic), a **Gensim** TF–IDF + similarity index, and a **PySpark** pipeline with LSH for scalable nearest-neighbor retrieval.\n\n- **Problem 2 (Clustering)**: Benchmarked **KMeans**, **GMM**, **Agglomerative** (scikit-learn) and **KMeans / BisectingKMeans / GMM** (PySpark).\n\n> You can now package the top-N similar items as an API or integrate into a UI. For segmentation, consider labeling clusters and mapping them to pricing/actions.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}